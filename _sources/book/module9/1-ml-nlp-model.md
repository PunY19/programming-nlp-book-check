# แบบจำลองการประมวลผลภาษาธรรมชาติ

การประมวลผลภาษาธรรมชาติ คือ เทคนิควิธีที่ใช้ในการประมวลผลและทำความเข้าใจข้อมูลตัวอักษร โดยอาศัยแบบจำลองทางภาษา แบบจำลองหรือโมเดลที่เราพูดถึงนี้หมายถึง การเขียนโปรแกรมที่จำลองการทำความเข้าใจภาษาในด้านต่าง ๆ ซึ่งการจำลองอาจจะไม่ได้สอดคล้องกับวิธีที่มนุษย์ทำความเข้าใจภาษา แต่ว่าแบบจำลองจำลองพฤติกรรมการประมวลผลภาษาได้ดีพอ ที่จะนำไปใช้ในการทำงานที่เกี่ยวข้องกับภาษาธรรมชาติได้ 
งานที่สามารถใช้โมเดลการประมวลผลภาษาธรรมชาติได้อย่างมีประสิทธิภาพ และพบเห็นได้บ่อย ๆ ได้แก่  เช่น การแท็กชนิดของคำ (part-of-speech tagging) การตรวจจับเอนติตี้ (named entity recognition) การวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) แบบจำลองหัวข้อ (topic model) การแปลด้วยเครื่อง (machine translation) ซึ่งทั้งหมดนี้คือต่างใช้การเรียนรู้ของเครื่อง (machine learning) ในการสร้างแบบจำลองสำหรับงานเหล่านี้ขึ้นมา กล่าวคือเราเขียนโปรแกรมที่เรียนรู้การทำงานเหล่านี้โดยการหาแพทเทิร์นจากชุดข้อมูลที่เหมาะสม เราไม่ได้ตั้งโปรแกรมให้ทำงานตามกฎของภาษาศาสตร์โดยตรง 

ในปัจจุบันเราสามารถดาวน์โหลดโมเดลและนำมาประยุกต์ใช้ได้ทันทีโดยที่ต้องไม่ทำการสร้างโมเดลใหม่ตั้งแต่ต้น แต่ว่าอาจจะมีโมเดลสำหรับบางงานและบางภาษาเท่านั้น และโมเดลของแต่ละภาษามีประสิทธิภาพ และความแม่นยำแตกต่างกันไป ในบทนี้เราจะเรียนรู้วิธีการใช้ไลบรารีเพื่อดาวน์โหลดและประยุกต์ใช้โมเดลในการวิเคราะห์ข้อมูลภาษาโดยอัตโนมัติ

## การแท็กชนิดของคำ (Part-of-speech tagging) 

ชนิดของคำ (part-of-speech tag) ในเชิงภาษาศาสตร์ถูกสร้างขึ้นมาเป็นส่วนหนึ่งของกฎไวยากรณ์ของภาษาที่กำหนดว่าคำชนิดใดสามารถมีความสัมพันธ์กับคำชนิดใด ในลักษณะไหนได้บ้าง  นอกจากนั้นแล้วยังมีบทบาทสำคัญในการทำความเข้าใจความหมายและโครงสร้างของประโยค
เนื่องจากช่วยให้เรารู้ว่าแต่ละคำในประโยคทำหน้าที่อะไร  เช่น คำนาม (noun) มักจะทำหน้าที่เป็นประธานของประโยค หรือเป็นส่วนเติมเต็มของกริยา หรีอ คุณศัพท์ (adjective) มักจะทำหน้าที่เป็นตัวขยายความหมายของคำนาม เป็นต้น 

```{margin} คำศัพท์
ชนิดของคำ (part-of-speech tag)
```

ในการประมวลผลภาษาธรรมชาติ ชนิดของคำมักจะช่วยในการสกัดข้อมูล (information extraction) จากข้อมูลที่เป็นข้อความ เช่น ถ้าหากเราต้องการสกัดเอาชื่อของบุคคลจากข้อความ เราอาจจะเลือกเอาเฉพาะคำที่เป็นคำนามมาพิจารณาเท่านั้น หรือถ้าหากเราต้องการสกัดคีย์เวิร์ดที่บ่งบอกถึงความหมายใจความสำคัญของคลังเอกสาร เราอาจจะเลือกเฉพาะไบแกรมที่มีคำนามและคำกริยามาพิจารณาเท่านั้น 


```{margin} คำศัพท์
การสกัดข้อมูล (information extraction)
```

### การแยกชนิดของคำ

ชนิดของคำมีชนิด ชนิดอะไรบ้าง ทฤษฎีการแยกชนิดของคำมีอยู่หลากหลายทฤษฎี เช่น เด็กนักเรียนไทยมักจะคุ้นเคยกับการแยกชนิดของคำตามพระยาอุปกิตศิลปสารซึ่งแยกชนิดของคำเป็น 7 ชนิด ได้แก่ คำนาม คำสรรพนาม คำวิเศษณ์ คำกริยา คำบุพบท คำสันธาน และคำอุทาน  ในการประมวลผลภาษาธรรมชาติ เรามักใช้ทฤษฎีของ ชุดป้ายระบุชนิดของคำสากล (Universal Part-of-speech Tagset) ทฤษฎีนี้กำหนดชนิดของคำออกเป็น 17 ชนิด ซึ่งสามารถจัดออกเป็น 3 กลุ่มใหญ่ ได้แก่ 

```{margin} คำศัพท์
ชุดป้ายระบุชนิดของคำสากล (Universal Part-of-speech Tagset) 
```


1. คำชนิดเปิด (open class word) คือกลุ่มคำที่สามารถเพิ่มคำใหม่เข้าไปได้เรื่อยๆ คำชนิดเปิดมักมีความหมายที่ชัดเจนและสามารถยืนอยู่ได้ด้วยตัวเอง เช่น การกระทำ คน สัตว์ สิ่งของ สถานที่ คำชนิดเปิดประกอบด้วยคำ 6 ชนิดย่อย
    - คำวิเศษณ์ (adverb)
    - คำคุณศัพท์ (adjective)
    - คำอุทาน (interjection)
    - คำนาม (noun)
    - คำกริยา (verb)
    - วิสามานยนาม หรือคำนามชื่อเฉพาะ (proper noun)
2. คำชนิดปิด (closed class word) เป็นกลุ่มคำที่มีจำนวนจำกัดและไม่ค่อยมีการเพิ่มคำใหม่ หน้าที่หลักของคำชนิดปิดคือการเชื่อมโยงหรือจัดระเบียบความสัมพันธ์ระหว่างคำในประโยค คำชนิดปิดมักมีความหมายที่น้อยลงหรือเป็นนามธรรม และมักไม่สามารถยืนอยู่ได้ด้วยตัวเอง แต่ต้องใช้งานร่วมกับคำอื่นเพื่อสร้างประโยคที่ถูกหลักไวยากรณ์ คำชนิดปิดประกอบด้วยคำ 8 ชนิดย่อย  
    - คำบุพบท (adposition)
    - คำช่วยกริยา (auxiliary)
    - ตัวกำหนด (determiner)
    - ตัวเลข (numeral)
    - คำอนุภาค (particle)
    - คำสรรพนาม (pronoun)
    - คำสันธานเชื่อมความ (coordinating conjunction)
    - คำสันธานซ้อนความ (subordinating conjunction)
3. คำชนิดอื่น ๆ ประกอบด้วยคำ 3 ชนิดย่อย 
    - เครื่องหมายวรรคตอน (punctuation)
    - สัญลักษณ์ (symbol)
    - อื่น ๆ ที่ไม่เข้าพวกกับคำชนิดที่กล่าวมาแล้ว

 {cite}` petrov-etal-2012-universal`     

ชุดป้ายระบุชนิดของคำสากลเป็นระบบการจำแนกชนิดของคำที่มีมาตรฐานเดียวกันทุกภาษา เพื่อให้การทำงานของโมเดลภาษาต่าง ๆ สามารถทำงานร่วมกันได้โดยสะดวกต่อการใช้โมเดลที่มีการใช้การถ่ายโอนข้ามภาษา (cross-lingual transfer) การใช้ชุดป้ายระบุชนิดของคำสากล จึงเป็นเครื่องมือที่สำคัญและได้รับความนิยมในการพัฒนาระบบประมวลผลภาษาธรรมชาติที่มีประสิทธิภาพ

## การตรวจจับเอนทิตี (Named Entity Recognition)

เอนทิตีในเชิงทฤษฎีแล้วคือ สิ่งที่มีตัวตนจริงและอิสระไม่เกี่ยวพันกับสิ่งอื่น ๆ ซึ่งเรามักจะหมายถึงบุคคล องค์กร บริษัท หรือสถานที่ เพราะฉะนั้นคำว่า named entity หมายความถึง บุคคล องค์กร สถานที่ หรือสิ่งที่มีตัวตนประเภทอื่น ๆ ที่มีสามารถอ้างถึงได้ด้วยชื่อ  ในบริบทของการประมวลผลภาษาธรรมชาติการตรวจจับเอนทิตี (Named Entity Recognition หรือ NER) คือ การตรวจจับชื่อของบุคคล ชื่อขององค์กร ชื่อของสถานที่ ชื่อของเหตุการณ์ หรือชื่อของสิ่งของที่มีความสำคัญออกมาจากข้อความ และในทางปฏิบัติแล้วการตรวจจับเอนทิตีไม่ได้จำกัดเพียงแต่เอนทิตีอย่างเดียว แต่หมายความรวมถึงการตรวจจับส่วนอื่นของข้อความที่มีความสำคัญ​และอาจจะไม่ได้มีตัวตนอิสระตามความหมายของเอนทิตี เช่น

```{margin} คำศัพท์
เอนทิตี (entity)
```

- เวลาและวันที่ เช่น "เมื่อวาน" หรือ "สัปดาห์หน้า"
- จำนวนเงิน เช่น "500 บาท" หรือ "หนึ่งพันเหรียญสหรัฐ"
- จำนวนและขนาด เช่น "3 กิโลเมตร" หรือ "5 ลิตร"
- คำนำหน้าชื่อ และคำศัพท์ทั่วไปที่เป็นคำบ่งบอกสถานที่ เช่น "เด็กชาย" "นางสาว" หรือ "โรงเรียน" ซึ่งไม่ได้บ่งบอกถึงชื่อเฉพาะของบุคคลหรือสถานที่

การตรวจจับเอนทิตีมีประโยชน์อย่างมากในหลายด้านของการประมวลผลภาษาธรรมชาติ และการประยุกต์ใช้ในงานต่างๆ โดยสามารถยกตัวอย่างได้ดังนี้ 

- ในการวิเคราะห์ความเห็นของลูกค้าเกี่ยวกับผลิตภัณฑ์ในโซเชียลมีเดีย บริษัทสามารถใช้การตรวจจับเอนทิตีเพื่อระบุชื่อผลิตภัณฑ์ ชื่อบริษัท หรือบุคคลที่เกี่ยวข้อง จากนั้นสามารถวิเคราะห์ความเห็นในเชิงบวกหรือลบที่มีต่อเอนทิตีเหล่านี้ได้อย่างมีประสิทธิภาพ ซึ่งช่วยให้บริษัทสามารถเข้าใจความคิดเห็นของลูกค้าเกี่ยวกับผลิตภัณฑ์หรือบริการได้อย่างละเอียด และสามารถปรับปรุงผลิตภัณฑ์และการบริการตามความคิดเห็นที่ได้รับได้อย่างตรงจุด 
- ในระบบตอบคำถามอัตโนมัติ เช่น ระบบบริการหลังการขายออนไลน์  การตรวจจับเอนทิตีสามารถระบุข้อมูลสำคัญจากคำถามของผู้ใช้ เช่น ชื่อผลิตภัณฑ์ หรือปัญหาที่เกิดขึ้น ทำให้ระบบสามารถให้คำตอบที่แม่นยำและรวดเร็วแก่ผู้ใช้ ลดเวลาที่ต้องใช้ในการค้นหาข้อมูลและเพิ่มประสิทธิภาพในการให้บริการลูกค้า
- ในด้านการสกัดข้อมูลทางการแพทย์ การตรวจจับเอนทิตีมีบทบาทสำคัญในการวิเคราะห์เวชระเบียนหรือเอกสารทางการแพทย์ โดยสามารถระบุชื่อโรค ชื่อยา ชื่อแพทย์ หรือวันนัดหมายได้อย่างชัดเจน ซึ่งช่วยในการจัดการข้อมูลทางการแพทย์อย่างเป็นระบบ สามารถสกัดข้อมูลสำคัญเพื่อใช้ในการวินิจฉัยโรค หรือการวิจัยทางการแพทย์ได้อย่างรวดเร็วและแม่นยำ 
- ในการวิเคราะห์ข่าวสารจากแหล่งต่างๆ การตรวจจับเอนทิตีสามารถระบุชื่อบุคคลสำคัญ เหตุการณ์สำคัญ สถานที่ หรือชื่อองค์กรที่ปรากฏในข่าว ทำให้สำนักข่าวหรือองค์กรที่เกี่ยวข้องสามารถติดตามและวิเคราะห์ข้อมูลข่าวสารได้อย่างมีประสิทธิภาพ สามารถสรุปแนวโน้มข่าวสาร หรือทำการวิจัยเชิงลึกเกี่ยวกับประเด็นที่สำคัญได้ 


การตรวจจับเอนทิตีมักจะใช้เทคนิคการประมวลผลภาษาธรรมชาติเช่นเดียวกับการแยกชนิดของคำ โดยใช้โมเดลการเรียนรู้ของเครื่องที่เรียนรู้จากชุดข้อมูลที่มีเอนทิตีที่ถูกติดป้ายไว้ และใช้เอนทิตีที่ติดป้ายเหล่านั้นในการสร้างโมเดลที่สามารถตรวจจับเอนทิตีใหม่ได้

## ไลบรารี spacy สำหรับการติดป้ายชนิดของคำ และการตรวจจับเอนทิตี

การติดป้ายชนิดของคำ และการตรวจจับเอนทิตีอัตโนมัติจำเป็นต้องใช้โมเดลแบบการเรียนรู้ของเครื่องในการแยก เพราะฉะนั้นไลบรารีที่ใช้จะต้องมีโมเดลเหล่านี้เตรียมไว้ให้พร้อมใช้ ไลบรารี spacy เป็นไลบรารีที่รองรับการประมวลผลภาษาธรรมชาติหลากหลายภาษา รวมถึงมีแบบจำลองการประมวลผลภาษาธรรมชาติหลากหลายขนาดให้เลือกใช้ตามความเหมาะสม และตามข้อจำกัดในการนำแบบจำลองไปใช้จริง  {cite}`spacy` ปัจจุบัน (2024) spacy รองรับการติดป้ายชนิดของคำสำหรับภาษาหลายภาษา เช่น ภาษาอังกฤษ ภาษาสเปน ภาษาฝรั่งเศส ภาษาจีน ภาษาญี่ปุ่น ภาษาเกาหลี แต่ว่า spacy ยังไม่มีโมเดลในการประมวลผลภาษาไทย

ขั้นแรกต้องหาชื่อชุดโมเดลที่เราต้องการใช้ รายชื่อชุดโมเดลอยู่บนเว็บไซต์ของ spacy ซึ่งมีเอกสารประกอบการใช้งานที่เป็นปัจจุบันอยู่ โมเดลเหล่านี้ประกอบไปด้วยโมเดลย่อยหลาย ๆ โมเดลด้วยกัน เราสามารถตรวจสอบดูบนเว็บไซต์ได้ว่าโมเดลแต่ละชุดประกอบด้วยโมเดลอะไรบ้าง  ตัวอย่างเช่น สำหรับภาษาจีน มีชุดโมเดลที่มีโมเดลการแยกชนิดของคำได้อยู่สามแบบจำลองได้แก่ `zh_core_web_sm` (ขนาดเล็ก) `zh_core_web_md` (ขนาดกลาง) และ  `zh_core_web_lg` (ขนาดใหญ่) ทั้งสามชุดในเวอร์ชันปี 2024 ประกอบไปด้วย 5 โมเดลย่อย ได้แก่ โมเดลการตัดคำ โมเดลการตัดประโยค โมเดลการวิเคราะห์โครงสร้างประโยค โมเดลการติดป้ายชนิดของคำ โมเดลการตรวจจับเอนทิตี  

หากเรายังไม่ได้ติดตั้งไลบรารี spacy เราสามารถติดตั้งโดยใช้คำสั่ง `pip install spacy` หลังจากนั้นเราสามารถดาวน์โหลดและติดตั้งแบบจำลองที่ต้องการได้โดยใช้คำสั่ง `python -m spacy download` ตามด้วยชื่อแบบจำลองที่ต้องการดาวน์โหลดและติดตั้ง เช่น

```bash
python -m spacy download zh_core_web_md
```

วิธีการใช้ spacy จะต่างจากการใช้ pythainlp ตรงที่ว่าเราจะต้องสร้างอ็อบเจกต์ที่ทำหน้าที่เป็นตัวประมวลผลที่โหลดโมเดลที่ต้องการมาให้พร้อม เพื่อที่จะได้ไม่ต้องโหลดโมเดลซ้ำ ๆ บ่อย ๆ เพราะถ้าหากเป็นโมเดลขนาดใหญ่แล้วต้องโหลดโมเดลซ้ำ ๆ จะทำให้โปรแกรมทำงานช้า ไม่เหมาะกับการประมวลผลข้อมูลขนาดใหญ่ที่เราอาจจะต้องลูปวนเพื่อประมวลผลข้อมูลทีละชิ้น ตัวอย่างการใช้งาน spacy ในการแยกชนิดของคำแสดงในโค้ดต่อไปนี้

```python 
import spacy
model_name = 'zh_core_web_md'
nlp_processor = spacy.load(model_name)
text  = '张伟和李娜在漂亮的上海吃饭'
doc = nlp_processor(text)
for token in doc:
    print(token.text, token.pos_)
```

ผลลัพธ์ที่ได้จะแสดงชนิดของคำที่แยกออกมาจากข้อความ ตามที่กำหนดไว้ในชุดป้ายระบุชนิดของคำสากลดังนี้

```
张伟 PROPN
和 CCONJ
李娜 PROPN
在 ADP
漂亮 VERB
的 PART
上海 PROPN
吃饭 VERB
```

 ในตัวอย่างนี้เราโหลดแบบจำลอง `zh_core_web_md` ซึ่งเป็นแบบจำลองขนาดกลางสำหรับภาษาจีน โดยการเรียกคำสั่ง `spacy.load` เพื่อสร้างตัวประมวลผลข้อความและเก็บไว้ในตัวแปรชื่อ `nlp_processor` ซึ่งสามารถใช้ได้คล้ายกับว่าเป็นฟังก์ชัน เราจึงสามารถเรียก `nlp_processor(text)` ได้เลยโดยไม่ต้องใช้ชื่อเมท็อดอื่น ผลที่ได้คือเป็น `Doc` อ็อบเจกต์ที่เราสามารถวนลูปเพื่อดึง `Token` อ็อบเจกต์ออกมาและดึงชนิดของคำออกมาได้โดยใช้ `token.pos_` ในการเข้าถึงชนิดของคำ


 ข้อสังเกตที่สำคัญอีกประการหนึ่งของการใช้ spacy คือ ไลบรารีนี้ประมวลข้อความเป็นแบบการทำงานแบบสายท่อ (pipeline) กล่าวคือสตริงที่รับเข้ามาจะถูกประมวลหลาย ๆ ขั้นในครั้งเดียว เช่น การประมวลภาษาจีนจะเริ่มด้วยการตัดคำ ต่อด้วยการแยกชนิดของคำ การวิเคราะห์โครงสร้างประโยค และจบด้วยการตรวจจับเอนทิตี จากนั้นผู้ใช้สามารถเข้าถึงผลการวิเคราะห์ทั้งหมดนี้ผ่าน `Doc` อ็อบเจกต์ที่ได้รับกลับมาจากการเรียก `nlp_processor(text)` โดยไม่ต้องเรียกแต่ละขั้นตอนเอง  เพราะฉะนั้นเราสามารถเข้าถึงรายชื่อเอนทิตีได้เลย โดยไม่ต้องประมวลผลข้อความอีกครั้ง

 ```python
for ent in doc.ents:
    print(ent.text, ent.label_)
```

ผลลัพท์ที่ได้จะแสดงชื่อเอนทิตีที่ตรวจจับออกมาจากข้อความดังนี้

```
张伟 PERSON
李娜 PERSON
上海 GPE
```
จากข้อความตัวอย่างที่ป้อนเข้าไป ตัวประมวลผลตรวจจับชื่อเอนทิตีที่เป็นบุคคลทั้งหมดสองชื่อ ได้แก่  张伟 (จางเหว่ย) และ 李娜 (หลีน่า) และเอนทิตีแบบภูมิรัฐศาสตร์ (GPE ซึ่งย่อมาจาก geopolitical entity) หนึ่งชื่อซึ่งก้คือ 上海 (เมืองเซี่ยงไฮ้)


## การวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis)

การวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) คือกระบวนการในการตรวจจับและจำแนกอารมณ์หรือความรู้สึกที่แสดงออกมาในข้อความ โดยใช้เทคนิคทางด้านการประมวลผลภาษาธรรมชาติ และการเรียนรู้ของเครื่อง การวิเคราะห์อารมณ์ความรู้สึกมักถูกใช้เพื่อประเมินความคิดเห็นของผู้ที่ใช้งานระบบ หรือผู้บริโภคที่มีต่อผลิตภัณฑ์ บริการ รวมถึงการประเมินความคิดเห็นของบุคคลทั่วไปที่มีต่อเหตุการณ์ต่าง ๆ โดยสามารถจำแนกออกเป็นอารมณ์เชิงบวก เชิงลบ หรือเป็นกลาง การวิเคราะห์อารมณ์ความรู้สึกนับว่าเป็นการประยุกต์ใช้การประมวลผลภาษาธรรมชาติในโลกของวิทยาการข้อมูลที่เห็นเด่นชัด และเป็นที่นิยมที่สุดชนิดหนึ่งในปัจจุบัน ตัวอย่างของการวิเคราะห์อารมณ์ความรู้สึกมีอยู่หลากหลายแบบ เช่น

```{margin} คำศัพท์
การวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) 
```

- การวิเคราะห์ความคิดเห็นของลูกค้าในโซเชียลมีเดีย: สมมติว่าเรามีข้อความจากทวิตเตอร์เกี่ยวกับผลิตภัณฑ์ใหม่ของบริษัท เราสามารถแยกแยะความคิดเห็นเชิงบวก ลบ หรือกลาง และนับจำนวนความคิดเห็นแต่ละประเภทได้ และแยกวิเคราะห์ได้ บริษัทสามารถใช้ข้อมูลนี้เพื่อเข้าใจความคิดเห็นของลูกค้าต่อผลิตภัณฑ์ใหม่ และนำไปปรับปรุงผลิตภัณฑ์หรือกลยุทธ์การตลาดได้

- การประเมินความพึงพอใจของลูกค้าในบทวิจารณ์ออนไลน์:
การวิเคราะห์อารมณ์ความรู้สึกสามารถแยกแยะได้ว่าอาจจะมีความรู้สึกเชิงบวกต่อผลิตภัณฑ์ แต่มีความรู้สึกเชิงลบต่อการบริการ ร้านอาหารสามารถใช้ข้อมูลนี้เพื่อปรับปรุงการบริการและรักษาคุณภาพอาหารให้ดีอยู่เสมอ

- การวิเคราะห์อารมณ์ความรู้สึกที่อยู่เอกสารทางการเงิน ทุก ๆ ปีบริษัทที่มีการซื้อขายหุ้นอยู่ในตลาดหลักทรัพย์ จะต้องเผยแพร่เอกสารรายงานผลประกอบการที่จะต้องพูดถึงบริษัทในหลากหลายด้าน เช่น เรื่องผลกำไรขาดทุน ความเสี่ยงในการประกอบธุรกิจ การควบรวมกับบริษัทอื่น การพัฒนากำลังคน หรือการเปลี่ยนแปลงในโครงสร้างบริษัท นักวิเคราะห์สามารถวิเคราะห์อารมณ์ความรู้สึกในเอกสารรายงานผลประกอบการ เพื่อให้ได้ดัชนีในการชีวัดว่าผู้เขียนมีความคิดเห็นในด้านบวก ลบ หรือกลางในด้านใดบ้างในปีที่ผ่านมา ซึ่งมีการศึกษามาว่าผลการวิเคราะห์ในลักษณะดังกล่าวมีความสัมพันธ์ร่วมกันกับผลตอบแทนเกินปกติ (abnormal return) ดังนั้นการวิเคราะห์อารมณ์ความรู้สึกจึงสามารถช่วยให้นักลงทุนหรือผู้บริหารบริษัทมีดัชนีชี้วัดเพิ่มอีกตัวหนึ่งที่ช่วยในการตัดสินใจลงทุนในสินทรัพย์ต่าง ๆ ได้อย่างมีประสิทธิภาพมากขึ้น 

- การติดตามความคิดเห็นเรื่องการเมืองบนสื่อออนไลน์: การศึกษาเรื่องวิเคราะห์อารมณ์ความรู้สึกที่อยู่บนสื่อออนไลน์ เช่น เว็บไซต์ reddit สามารถช่วยให้นักวิเคราะห์เข้าใจความคิดเห็นของประชาชนต่อเรื่องการเมือง เนื่องจากเหตุการณ์ทางการเมืองต่าง ๆ ส่งผลต่ออารมณ์ความรู้สึกที่ปรากฏอยู่ในสื่อเหล่านี้ {cite}`tachaiya2021raffman` การวิเคราะห์อารมณ์ความรู้สึกบนฟอรัมออนไลน์มีประโยชน์อย่างมากในการทำความเข้าใจและตอบสนองต่อความคิดเห็นของประชาชนได้อย่างมีประสิทธิภาพ โดยช่วยให้เราสามารถติดตามการเปลี่ยนแปลงของอารมณ์ความรู้สึกในช่วงเวลาต่างๆ เมื่อเกิดเหตุการณ์สำคัญ เช่น การถอดถอนประธานาธิบดี ซึ่งจะเป็นประโยชน์ในการระบุแนวโน้มทางการเมืองและสังคม นอกจากนี้ ยังสามารถช่วยในการวิเคราะห์แนวโน้มการแบ่งขั้วทางการเมืองหรือความรู้สึกที่มีต่อประเด็นสำคัญในสังคมอีกด้วย 

การวิเคราะห์อารมณ์ความรู้สึกในเชิงปฏิบัติ คือ การแยกประเภทของข้อความว่าเป็นข้อความที่แสดงความเห็นในด้านบวก ด้านลบ หรือเป็นกลาง เราสามารถคิดว่าตัววิเคราะห์อารมณ์ความรู้สึก (sentiment analyzer) เป็นฟังก์ชันที่รับสตริงข้อความ และคืนค่าเป็นป้ายกำกับ (label) ที่บ่งบอกว่าข้อความนั้นเป็นเชิงบวก ลบ หรือเป็นกลาง โดยใช้เทคนิคการเรียนรู้ของเครื่อง ซึ่งสามารถทำได้ด้วยการใช้ชุดข้อมูลที่มีป้ายระบุความรู้สึกเช่นข้อความที่มีป้ายระบุว่าเป็นเชิงบวก ลบ หรือเป็นกลาง ในการสร้างโมเดลการเรียนรู้ของเครื่อง โดยใช้ข้อมูลเหล่านี้ในการเรียนรู้ว่าคุณลักษณะของข้อความที่เป็นเชิงบวก ลบ หรือกลาง คืออะไร และใช้คุณลักษณะเหล่านั้นในการจำแนกประเภทของข้อความใหม่ที่ยังไม่เคยเห็นมาก่อน หนึ่งในชุดข้อมูลที่ได้รับความนิยมมากในการนำมาสร้างตัววิเคราะห์อารมณ์ความรู้สึก คือ Sentiment Treebank  ซึ่งนำมาแสดงให้ดูเป็นตัวอย่างดังนี้ {cite}`sentimenttreebank`

|  | ข้อความ | ป้ายกำกับ |
|---|---------|-------------------|
|(a) | Leguizamo and Jones are both excellent and the rest of the cast is uniformly superb. | บวก |
|(b) | Still, this flick is fun, and host to some truly excellent sequences. | บวก |
|(c) | A potentially good comic premise and excellent cast are terribly wasted. | ลบ |
|(d)| There's an excellent 90-minute film here; unfortunately, it runs for 170 | ลบ | 
|(e)| So brisk is Wang's pacing that none of the excellent cast are given air to breathe. | เป็นกลาง |

ข้อสังเกตที่สำคัญอย่างหนึ่งจากตัวอย่างข้างต้น คือ ถึงแม้ว่าทุกข้อความจะมีคำว่า *excellent* อยู่ ซึ่งแปลว่ายอดเยี่ยม แต่ก็ไม่ได้หมายความว่าอารมณ์ความรู้สึกที่ส่งผ่านข้อความนั้นจะเป็นบวกเสมอไป ยังมีปัจจัยทางบริบทที่มีผลต่อการตีความโดยรวม เช่น บริบททางโครงสร้างประโยคในตัวอย่าง (c) นำเอาคำว่า *excellent* มาใช้เป็นส่วนขยายของนามแต่ว่าไม่ได้นำมาใช้เป็นส่วนหนึ่งของกริยาหลัก ซึ่งมีผลต่อความหมายโดยรวมของประโยคมากกว่า ทำให้เห็นว่าเราต้องคำนึงปัจจัยทางภาษาศาสตร์อื่น ๆ อีกมาก นอกเหนือจากความหมายของคำ จึงจะสามารถวิเคราะห์อารมณ์ความรู้สึกได้ถูกต้อง

## แพลตฟอร์ม huggingface และไลบรารี transformers สำหรับการวิเคราะห์อารมณ์ความรู้สึก 
Huggingface เป็นแพลตฟอร์ม (ระบบที่ทุกคนเข้าใช้ร่วมกัน แทนที่จะมีคนกลุ่มเดียวที่เข้าใช้ได้) ที่มุ่งเน้นการพัฒนาและแบ่งปันโมเดลการประมวลผลภาษาธรรมชาติที่มีความสามารถหลากหลาย กล่าวคือนักพัฒนาสามารถนำโมเดลที่ตนสร้างขึ้นมาฝากไว้บนแพลตฟอร์มนี้ เพื่อให้ผู้ใช้คนอื่นเข้ามาโหลดเพื่อนำไปใช้ได้อย่างสะดวก ซึ่งหน้าที่ของแพลตฟอร์มคืออำนวยความสะดวกให้ทั้งสองฝ่ายโดยไม่ต้องพัฒนาโมเดลทั้งหมดเอง  
```{margin} คำศัพท์
แพลตฟอร์ม (platform) 
```

แพลตฟอร์ม huggingface อยู่บนเว็บไซต์ https://huggingface.co  และมาพร้อมกับไลบรารีที่เรียกว่า transformers ซึ่งเป็นไลบรารีไพธอนที่ช่วยโหลดโมเดลมาจากแพลตฟอร์ม เพื่อนำไปพัฒนาเป็นระบบอื่น ๆ ในภาษาไพธอนได้อย่างสะดวก huggingface รวมโมเดลที่ถูกพัฒนามาแล้วหลากหลายรุ่นที่สามารถนำไปใช้ในงานต่าง ๆ ได้ทันที ถ้าหากเราค้นหาโมเดล sentiment analysis บนแพลตฟอร์ม ก็จะพบว่ามีโมเดลให้เลือกกว่า 100 โมเดล เราจำเป็นต้องดูว่าโมเดลใดรองรับภาษาที่เราต้องการวิเคราะห์ รวมถึงตรวจสอบอีกว่าความแม่นยำที่คาดหวังได้คือเท่าไร ผู้ที่นำโมเดลมาฝากในแพลตฟอร์มนี้มักจะมีเขียนไว้อย่างละเอียดถึงกระบวนการพัฒนาโมเดล เหมาะกับข้อมูลประเภทใด (เช่น ทวีต รีวิว หรือข่าวการเงิน) และความแม่นยำตามที่ได้ประเมินสิทธิภาพไว้ 

เริ่มต้นจากการติดตั้งไลบรารีด้วยคำสั่ง `pip install transformers` วิธีการใช้ไลบรารีนี้แบบพื้นฐานที่สุด ทำได้โดยการใช้ฟังก์ชัน `pipeline` ในการโหลดโมเดลและปรับการใช้งาน โดยที่เราจะต้องทราบชื่อโมเดลที่ต้องการใช้ 
โมเดลที่แนะนำในปัจจุบัน (มิถุนายน 2567) สำหรับการวิเคราะห์อารมณ์ความรู้สึกภาษาไทยคือ `poom-sci/WangchanBERTa-finetuned-sentiment` ซึ่งเรียนรู้การวิเคราะห์มาจากชุดข้อมูลที่เป็นรีวิว รวมกันกับข้อมูลที่เป็นทวีต ทำให้เหมาะกับการวิเคราะห์เบื้องต้น ไม่ได้เฉพาะเจาะจงกับข้อมูลแหล่งใดแหล่งหนึ่ง 

```python
# Use a pipeline as a high-level helper
from transformers import pipeline
pipe = pipeline("text-classification", model="poom-sci/WangchanBERTa-finetuned-sentiment")
pipe(["เนื้อครีมลื่นไม่เหนียว", "กระปุกสีขาว ฝาเกลียว", "เปิดออกมาแล้วกลิ่นค่อนข้างแรง"])
```

ผลลัพธ์ที่ได้ออกมาคือ ลิสต์ของดิกชันนารีที่ประกอบด้วยป้ายกำกับ (label) ที่มีค่าความน่าจะเป็นสูงสุดคือป้ายกำกับที่ และค่าความน่าจะเป็น (score) ที่ได้ ดังนี้

```python
[{'label': 'pos', 'score': 0.6775356531143188},
 {'label': 'neu', 'score': 0.9761248230934143},
 {'label': 'neg', 'score': 0.7795576453208923}]
```

ภาษาไทยทีโมเดลอีกตัวหนึ่งซึ่งสร้างขึ้นมาเพื่อข้อมูลที่มาจากเอกสารทางการเงิน เช่น รายงานผลประกอบการ หรือข่าวทางการเงิน โมเดลตัวนี้ชื่อว่า `nlp-chula/augment-sentiment-finnlp-th` บนแพลต์ฟอร์ม huggingface เราสามารถนำมาใช้งานได้ด้วยคำสั่งเดียวกันเพียงแต่เปลี่ยนชื่อโมเดลที่ใช้ 

```python
pipe_finance_news = pipeline("text-classification", model="nlp-chula/augment-sentiment-finnlp-th")
pipe_finance_news([
    "บริษัทยังคงดำเนินนโยบายการขยายเครือข่ายสาขาอย่างต่อเนื่อง", 
     "ในช่วงคร่ึงปีหลัง บริษัทเน้น เรื่องการเร่งรัดติดตามทวงถามหนี้เพิ่มมากขึ้น",
    "การแพร่ระบาดของไวรัสโควิด-19 ระลอกใหม่เป็นปัจจัย สาคัญที่กดดันต่อการบริโภคภาคเอกชน และการท่องเท่ียว",
    "ธนาคารมีสินทรัพย์สภาพคล่องประมาณ 47,272 ล้านบาท ลดลงจานวน 2,424 ล้านบาท ",
    "ธุรกิจกองทุนสํารองเลี้ยงชีพของ บลจ. ทิสโก้ยังคงเติบโตกว่ําร้อยละ 11.8 เทียบกับ อุตสําหกรรมที่เติบโตลดลงเหลือเพียงร้อยละ 2.1 จากปี 2562"])
```

ผลลัพธ์ที่ได้ออกมาคือ 

```python
[{'label': 'Neutral', 'score': 0.6528419256210327},
 {'label': 'Neutral', 'score': 0.937697172164917},
 {'label': 'Negative', 'score': 0.9270865321159363},
 {'label': 'Negative', 'score': 0.8279978036880493},
 {'label': 'Positive', 'score': 0.5197721719741821}]
 ```

ข้อควรระวังอย่างหนึ่งของการวิเคราะห์อารมณ์ความรู้สึก คือการเลือกใช้โมเดลให้เหมาะสมกับขอบเขตเนื้อหา (domain) ของข้อมูลที่เราต้องการวิเคราะห์  ถ้าหากชุดข้อมูลที่ใช้พัฒนาโมเดลมีขอบเขตเนื้อหาที่ไม่ตรงกับ ขอบเขตเนื้อหาของข้อมูลที่เราต้องการวิเคราะห์ อาจจะทำให้ความแม่นยำลดลง หรือผลลัพธ์ที่ได้ไม่เป็นที่พอใจ สมมติว่าโมเดลการวิเคราะห์อารมณ์ความรู้สึก (sentiment analysis) ถูกพัฒนามาด้วยข้อมูลรีวิวภาพยนตร์ ซึ่งข้อมูลนี้มีลักษณะเฉพาะ เช่น คำศัพท์และรูปแบบการใช้ภาษาที่เกี่ยวข้องกับการรีวิวภาพยนตร์ หากเรานำโมเดลนี้ไปใช้วิเคราะห์ความคิดเห็นเกี่ยวกับการเงินการลงทุน  โมเดลก็จะมีความแม่นยำลดลง เพราะลักษณะทางภาษาหรือขอบเขตเนื้อหาของข้อมูลรีวิวสมาร์ทโฟนอาจมีความแตกต่างจากข้อมูลความเห็นเกี่ยวกับการเงินการลงทุน  เรียกอีกอย่างว่า ปัญหาการกระจายตัวแบบออกนอกขอบเขต (out-of-distribution) ซึ่งเป็นปัญหาที่สำคัญในการใช้โมเดลการเรียนรู้ของเครื่องในงานปัญญาประดิษฐ์ {cite}`ood2023`

ในตัวอย่างการใช้ huggingface ข้างต้น เราได้ลองใช้โมเดลภาษาไทยสองตัวที่ถูกพัฒนาขึ้นมาจากชุดข้อมูลคนละขอบเขตกัน ตัวแรกสร้างขึ้นเพื่อใช้วิเคราะห์รีวิวและทวีต และโมเดลตัวที่สองสร้างขึ้นเพื่อใช้วิเคราะห์ข้อความที่ขอบเขตเนื้อหาคือการเงิน และผลประกอบการของบริษัท 
หากเราใช้โมเดลตัวแรกมาวิเคราะห์ข้อความที่พูดถึงเรื่องการเงิน จะได้ผลดังนี้

```python
[{'label': 'neu', 'score': 0.9173294305801392},
 {'label': 'neu', 'score': 0.9335467219352722},
 {'label': 'neu', 'score': 0.8750076293945312}, # ที่ถูกต้องคือ 'neg'
 {'label': 'neu', 'score': 0.9450038075447083}, # ที่ถูกต้องคือ 'neg'
 {'label': 'neu', 'score': 0.9596440196037292}] # ที่ถูกต้องคือ 'pos'
```

หากเราใช้โมเดลตัวที่สองมาวิเคราะห์ที่เป็นรีวิวจะได้ผลดังนี้

```python
[{'label': 'Neutral', 'score': 0.6709030270576477}, # ที่ถูกต้องคือ 'Positive'
 {'label': 'Positive', 'score': 0.5924004316329956},# ที่ถูกต้องคือ 'Neutral'
 {'label': 'Neutral', 'score': 0.8645471930503845}]# ที่ถูกต้องคือ 'Negative'
```

เมื่อโมเดลทั้งสองตัวถูกนำไปใช้กับข้อมูลที่ไม่ตรงกับขอบเขตเนื้อหาที่ใช้พัฒนา ความแม่นยำลดลง และผลลัพธ์ที่ได้ไม่เป็นที่พอใจ ดังนั้นการเลือกใช้โมเดลที่เหมาะสมกับข้อมูลที่ต้องการวิเคราะห์เป็นสิ่งสำคัญ โดยทั่วไปแล้วโมเดลที่ได้รับความนิยมบนแพลตฟอร์ม huggingface มักจะระบุว่าโมเดลถูกพัฒนาขึ้นมาด้วยชุดข้อมูลอะไร ที่มาของข้อความอยู่ในขอบเขตเนื้อหาอะไร เพื่อผู้ใช้จะได้นำไปประยุกต์ใช้ได้อย่างเหมาะสมกับข้อมูลที่ต้องการวิเคราะห์ 

## แบบจำลองหัวข้อ (topic model)

*under construction*

## การแปลด้วยเครื่อง (machine translation)

*under construction*


## แบบจำลองภาษาขนาดใหญ่ (LLM: Large Language Model)
*under construction*


## อ้างอิง
```{bibliography}
:filter: docname in docnames
:style: plain

```