{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# แบบฝึกหัด: การประมวลผลข้อความขั้นพื้นฐาน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## สรุปคำสั่งที่ควรทราบ\n",
    "\n",
    "### การติดตั้งไลบรารี pythainlp attacut และ nltk ก่อนใช้งาน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pythainlp\n",
    "!pip install attacut\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### การตัดคำภาษาไทย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ข้างนอก', 'สุกใส', ' ', 'ข้างใน', 'ต๊ะติ๊ง', 'โหน่ง']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pythainlp\n",
    "import attacut\n",
    "pythainlp.word_tokenize('ข้างนอกสุกใส ข้างในต๊ะติ๊งโหน่ง')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ข้าง', 'นอก', 'สุกใส', ' ', 'ข้าง', 'ใน', 'ต๊ะติ๊งโหน่ง']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythainlp.word_tokenize('ข้างนอกสุกใส ข้างในต๊ะติ๊งโหน่ง', engine='attacut')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ตัดคำภาษาอังกฤษ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/te/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['COVID-19',\n",
       " ',',\n",
       " 'which',\n",
       " 'caused',\n",
       " 'problems',\n",
       " 'worldwide',\n",
       " ',',\n",
       " 'is',\n",
       " 'still',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'in',\n",
       " 'the',\n",
       " 'U.S.A.',\n",
       " 'until',\n",
       " 'today',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tokenize.word_tokenize('COVID-19, which caused problems worldwide, is still a problem in the U.S.A. until today.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ตัดประโยคภาษาไทย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ระฆังดี ', 'ถึงแม้คนไม่ตีก็ดัง ', 'ระฆังไม่ดีไม่ตีก็ไม่ดัง']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythainlp.sent_tokenize('ระฆังดี ถึงแม้คนไม่ตีก็ดัง ระฆังไม่ดีไม่ตีก็ไม่ดัง')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ตัดประโยคภาษาอังกฤษ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Punkt knows that the periods in Mr. Smith and Johann S. Bach are not sentence boundaries.',\n",
       " 'But he was with Mrs.',\n",
       " 'Bond that week.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize('Punkt knows that the periods in Mr. Smith and Johann S. Bach are not sentence boundaries.  But he was with Mrs. Bond that week.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### โหลดคำหยุดภาษาไทย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ข้างนอก', 'สุกใส', ' ', 'ข้่าง', 'ต๊ะติ๊ง', 'โหน่ง']\n"
     ]
    }
   ],
   "source": [
    "tokens = pythainlp.word_tokenize('ข้างนอกสุกใส ข้่างในต๊ะติ๊งโหน่ง')\n",
    "stopset = set(pythainlp.corpus.thai_stopwords())\n",
    "tokens_no_stopwords = [t for t in tokens if t not in stopset]\n",
    "print(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### โหลดคำหยุดภาษาอังกฤษ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/te/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', ',', 'caused', 'problems', 'worldwide', ',', 'still', 'problem', 'U.S.A.', 'today', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize('COVID-19, which caused problems worldwide, is still a problem in the U.S.A. until today.')\n",
    "stopset = set(nltk.corpus.stopwords.words('english'))\n",
    "tokens_no_stopwords = [t for t in tokens if t not in stopset]\n",
    "print(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กรองเอาเครื่องหมายวรรคตอนออก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COVID-19', 'which', 'caused', 'problems', 'worldwide', 'is', 'still', 'a', 'problem', 'in', 'the', 'U.S.A.', 'until', 'today']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "patt = re.compile('[^ก-์0-9a-zA-Z]')\n",
    "tokens_no_punct = [t for t in tokens if not patt.match(t)]\n",
    "print(tokens_no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## แบบฝึกหัดการวิเคราะห์ความถี่ของคำ\n",
    "เขียนฟังก์ชันที่นับจำนวนคำภาษาไทยจากข้อความที่เก็บอยู่ในสตริง `news_article` โดยที่ต้องไม่นับ เครื่องหมายวรรคตอน ตัวเลข และคำหยุด (ถ้าหากไม่ได้ใช้แบบฝึกหัดนี้บน Google Colab ให้ใช้สตริงอะไรก็ได้ที่ตั้งค่าให้เป็นบทความที่มีความยาวตั้งแต่ 1000 คำขึ้นไป)\n",
    "1. สตริงที่ให้มีขนาดคลังศัพท์ (จำนวนคำแบบไม่นับคำซ้ำ) อยู่ที่เท่าไร\n",
    "2. คำใดพบบ่อยที่สุด 10 อันดับในสตริงที่ให้มา\n",
    "3. ไบแกรมใดพบบ่อยที่สุด 10 อันดับในสตริงที่ให้มา\n",
    "4. สร้างเมฆคำจากคำที่พบบ่อยที่สุด 40 อันดับ ให้ปรับขนาดของคำให้เท่ากับความถี่ของคำนั้นๆ ไลบรารีเมฆคำไม่มีฟอนต์ภาษาไทยมาให้ เพราะฉะนั้นเราต้องดาวน์โหลดฟอนต์ภาษาไทยมาวางที่โฟลเดอร์เดียวกัน และตั้งค่า `font_path=` ให้ชี้ไปยังไฟล์ที่เก็บฟอนต์ไทย\n",
    "\n",
    "สมมติว่าสตริง `news_article` เก็บข้อมูลข่าวจาก https://thestandard.co/us-prepares-to-offer-billionaire-tax/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "34368ba4908ea1be08ba769dfb7764ab7f8ead2384ebb5604cb86637573696f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
